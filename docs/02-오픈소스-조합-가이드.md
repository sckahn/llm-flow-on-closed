# [최종안] LLMFlow 엔터프라이즈 오픈소스 아키텍처 가이드

## 문서 정보

| 항목 | 내용 |
| :--- | :--- |
| **문서명** | LLMFlow 엔터프라이즈 오픈소스 아키텍처 가이드 |
| **버전** | 2.0 (검토 및 보완 완료) |
| **작성일** | 2025-12-13 |
| **핵심 철학** | "Dify를 OS처럼, 부족한 장기(Organ)는 최고 성능의 전용 모듈로 이식한다." |

-----

## 1\. 아키텍처 개요 (Overview)

이 아키텍처는 **중복을 배제**하고 **데이터 흐름의 병목을 최소화**하도록 설계되었습니다.

```mermaid
graph TD
    User[사용자] --> APIGateway[Apache APISIX]
    APIGateway --> Frontend[Dify Web / Custom UI]
    
    subgraph "Core Platform (Orchestration)"
        Frontend --> DifyAPI[Dify API Server]
        DifyAPI <--> LangGraph[LangGraph (복잡 추론)]
        DifyAPI --> Monitor[Langfuse (Observability)]
    end

    subgraph "Model Serving (Inference)"
        DifyAPI --> vLLM[vLLM (LLM Serving)]
        DifyAPI --> Reranker[TEI / Infinity (Reranking)]
        DifyAPI --> Embedding[TEI / Infinity (Embedding)]
    end

    subgraph "Data Pipeline (RAG)"
        DifyAPI --> Unstructured[Unstructured (ETL/OCR)]
        Unstructured --> LlamaIndex[LlamaIndex (Indexing)]
        LlamaIndex --> Milvus[(Milvus - Vector DB)]
        LlamaIndex --> Neo4j[(Neo4j - Graph DB)]
    end

    subgraph "LLMOps (Training)"
        MLflow[MLflow] --> LLaMAFactory[LLaMA-Factory]
        LLaMAFactory --> vLLM
    end
```

-----

## 2\. 레이어별 상세 구성 및 변경 사유

### 2.1 오케스트레이션 레이어 (Core)

| 컴포넌트 | 역할 | 선정/변경 사유 |
| :--- | :--- | :--- |
| **Dify** | 메인 플랫폼 | **유지.** 워크플로우, 프롬프트 관리, RAG 파이프라인의 중심. |
| **LangGraph** | 심화 에이전트 | **유지.** 순환(Loop) 구조 및 멀티 에이전트 제어 필수 도구. |

### 2.2 모델 서빙 레이어 (Inference) - *최적화*

**변경점:** 복잡한 Ray를 제거하고, 경량화된 고성능 서빙 도구로 통합.

| 컴포넌트 | 역할 | 선정/변경 사유 |
| :--- | :--- | :--- |
| **vLLM** | LLM 서빙 | **Ray 제거.** 단일/멀티 GPU 환경에서 vLLM만으로도 충분히 OpenAI 호환 API 서빙 가능. 운영 복잡도 대폭 감소. |
| **Infinity** 또는 **TEI** | 임베딩 & 리랭킹 | **추가.** vLLM은 생성(Generation) 전문. 임베딩(Embedding)과 \*\*리랭킹(Reranking)\*\*은 전용 추론 서버인 Infinity(빠름)나 TEI(HuggingFace Text Embeddings Inference)가 훨씬 효율적임. |

### 2.3 데이터 처리 레이어 (ETL & RAG) - *기능 추가*

**변경점:** 문서 파싱(OCR) 및 GraphRAG, 하이브리드 검색 강화.

| 컴포넌트 | 역할 | 선정/변경 사유 |
| :--- | :--- | :--- |
| **Unstructured** | ETL / 파싱 | **추가.** PDF, PPT, 표(Table) 등을 텍스트로 변환하는 전처리 도구 필수. (Dify 내장 파서보다 강력함) |
| **LlamaIndex** | RAG 로직 | **유지.** 고급 청킹 및 GraphRAG 구현을 위해 필수. |
| **Milvus** | Vector DB | **유지.** 대규모 데이터 처리에 적합. |
| **Neo4j** | Graph DB | **추가.** 앞서 논의된 **GraphRAG** 및 관계형 지식 탐색을 위해 필수적임. (LlamaIndex와 결합) |

### 2.4 LLMOps (Fine-Tuning) 레이어 - *실용성 강화*

**변경점:** DeepSpeed 직접 사용보다 사용성이 좋은 래퍼(Wrapper) 도구 추천.

| 컴포넌트 | 역할 | 선정/변경 사유 |
| :--- | :--- | :--- |
| **LLaMA-Factory** | 파인튜닝 | **DeepSpeed 대체.** DeepSpeed는 라이브러리일 뿐임. LLaMA-Factory는 WebUI를 제공하며 LoRA/QLoRA 학습을 매우 쉽게 설정 가능. 내부적으로 DeepSpeed 사용함. |
| **MLflow** | 실험 관리 | **유지.** 학습 로그 및 모델 버전 관리 표준. |

### 2.5 Observability & Security

| 컴포넌트 | 역할 | 선정/변경 사유 |
| :--- | :--- | :--- |
| **Langfuse** | 모니터링 | **유지.** Dify와 가장 궁합이 좋음. |
| **Keycloak** | 인증 | **유지.** 엔터프라이즈 표준. |
| **Apache APISIX** | 게이트웨이 | **유지.** AI 전용 플러그인 우수. |
| **Presidio** | PII 필터링 | **유지.** 개인정보보호 필수. |

-----

## 3\. 추천 배포 아키텍처 (Docker Compose 예시)

운영 복잡도를 낮춘 최적화된 구성입니다.

```yaml
version: '3.8'

services:
  # 1. Main Platform
  dify-api:
    image: langgenius/dify-api:latest
    environment:
      - RAG_ENGINE=external # 복잡한건 LlamaIndex로 위임
      - VECTOR_STORE=milvus
    depends_on:
      - db
      - redis

  # 2. Inference Engine (LLM)
  vllm-service:
    image: vllm/vllm-openai:latest
    command: --model meta-llama/Meta-Llama-3-70B-Instruct --api-key mysecret --gpu-memory-utilization 0.95
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # 3. Inference Engine (Embedding & Reranker) -> 추가됨!
  tei-embedding:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-latest
    command: --model-id BAAI/bge-m3 --port 80
  
  tei-reranking:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-latest
    command: --model-id BAAI/bge-reranker-v2-m3 --port 80

  # 4. Data Stores
  milvus:
    image: milvusdb/milvus:v2.4-latest
  
  neo4j: # GraphRAG용 추가됨!
    image: neo4j:5.15.0
    environment:
      - NEO4J_AUTH=neo4j/password

  # 5. Ops & Monitoring
  langfuse:
    image: langfuse/langfuse:latest
  
  llama-factory: # Fine-tuning용 추가됨!
    image: hiyouga/llama-factory:latest
    volumes:
      - ./data:/app/data
```

-----

## 4\. 최종 스택 요약 및 선정 근거

| 분류 | 도구 | GitHub Stars | 선정 핵심 이유 |
| :--- | :--- | :--- | :--- |
| **Core** | **Dify** | \~118K | 대체 불가능한 올인원 LLM 앱 빌더 |
| **Agent** | **LangGraph** | \~12K | Dify의 한계(루프, 상태관리)를 완벽히 보완 |
| **Serving** | **vLLM** | \~65K | 압도적인 추론 속도와 OpenAI API 호환성 |
| **Rerank** | **TEI / Infinity** | \~10K | **(신규)** RAG 검색 품질의 핵심인 Reranking 전용 고속 서버 |
| **ETL** | **Unstructured** | \~9K | **(신규)** 문서 전처리(OCR, 표 인식)의 표준 |
| **RAG** | **LlamaIndex** | \~44K | GraphRAG 및 고급 검색 전략 구현체 |
| **VectorDB** | **Milvus** | \~40K | 10억 개 이상의 벡터 처리가 검증된 DB |
| **GraphDB** | **Neo4j** | \~12K | **(신규)** GraphRAG 구현을 위한 업계 표준 DB |
| **Train** | **LLaMA-Factory** | \~30K | **(변경)** 복잡한 DeepSpeed 코딩 없이 WebUI로 파인튜닝 가능 |
| **Observe** | **Langfuse** | \~19K | 오픈소스 중 가장 완성도 높은 트레이싱 도구 |

