# LLMFlow 인프라 구성 가이드

> **버전**: 2.0
> **최종 업데이트**: 2025년 1월
> **주요 변경사항**: Ray 제거, TEI/Infinity 추가, Unstructured 추가, Neo4j 추가, LLaMA-Factory 도입

## 1. 개요

### 1.1 목적

본 문서는 500명 이상의 사용자를 지원하는 엔터프라이즈 LLMFlow 플랫폼의 인프라 구성 가이드를 제공합니다.

### 1.2 설계 원칙

| 원칙 | 설명 |
|------|------|
| **고가용성 (HA)** | 단일 장애점 제거, 99.9% 가용성 목표 |
| **확장성** | 수평/수직 확장을 통한 부하 대응 |
| **보안** | 폐쇄망 환경, 네트워크 격리, 암호화 |
| **운영 효율성** | 자동화된 배포, 모니터링, 복구 |
| **비용 효율성** | 리소스 최적화, 멀티테넌시 활용 |

---

## 2. 하드웨어 사양

### 2.1 전체 클러스터 구성

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        LLMFlow 클러스터 토폴로지                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Management Plane (3 Nodes)                       │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ Master-01   │  │ Master-02   │  │ Master-03   │                  │   │
│  │  │ K8s Control │  │ K8s Control │  │ K8s Control │                  │   │
│  │  │ etcd        │  │ etcd        │  │ etcd        │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Application Plane (9+ Nodes)                     │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ App-01      │  │ App-02      │  │ App-03      │                  │   │
│  │  │ Dify        │  │ Dify        │  │ APISIX      │                  │   │
│  │  │ LangGraph   │  │ LangGraph   │  │ Keycloak    │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ App-04      │  │ App-05      │  │ App-06      │                  │   │
│  │  │ Langfuse    │  │ MLflow      │  │ Monitoring  │                  │   │
│  │  │ Presidio    │  │ MinIO       │  │ OpenSearch  │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ App-07      │  │ App-08      │  │ App-09      │                  │   │
│  │  │ TEI/Infinity│  │ Unstructured│  │ LLaMA-      │                  │   │
│  │  │ (Embedding) │  │ (ETL/OCR)   │  │ Factory     │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Data Plane (5+ Nodes)                            │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ Data-01     │  │ Data-02     │  │ Data-03     │                  │   │
│  │  │ PostgreSQL  │  │ PostgreSQL  │  │ PostgreSQL  │                  │   │
│  │  │ (Primary)   │  │ (Replica)   │  │ (Replica)   │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  │  ┌─────────────┐  ┌─────────────┐                                   │   │
│  │  │ Data-04     │  │ Data-05     │                                   │   │
│  │  │ Redis       │  │ Redis       │                                   │   │
│  │  │ (Master)    │  │ (Replica)   │                                   │   │
│  │  └─────────────┘  └─────────────┘                                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Vector/Graph DB Plane (9+ Nodes)                 │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ Milvus-01   │  │ Milvus-02   │  │ Milvus-03   │                  │   │
│  │  │ Query Node  │  │ Query Node  │  │ Data Node   │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ Milvus-04   │  │ Milvus-05   │  │ Milvus-06   │                  │   │
│  │  │ Data Node   │  │ Index Node  │  │ Coord/etcd  │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ Neo4j-01    │  │ Neo4j-02    │  │ Neo4j-03    │                  │   │
│  │  │ Core (Ldr)  │  │ Core        │  │ Core        │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     GPU Plane (8+ Nodes)                             │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │   │
│  │  │ GPU-01      │  │ GPU-02      │  │ GPU-03      │  │ GPU-04      │ │   │
│  │  │ A100 x8     │  │ A100 x8     │  │ A100 x8     │  │ A100 x8     │ │   │
│  │  │ Inference   │  │ Inference   │  │ Inference   │  │ Inference   │ │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │   │
│  │  │ GPU-05      │  │ GPU-06      │  │ GPU-07      │  │ GPU-08      │ │   │
│  │  │ A100 x8     │  │ A100 x8     │  │ H100 x8     │  │ H100 x8     │ │   │
│  │  │ Training    │  │ Training    │  │ Training    │  │ Training    │ │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 노드별 상세 사양

#### 2.2.1 Management Plane (Kubernetes Control Plane)

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | Intel Xeon Gold 6348 (28C/56T) | 2 | 고가용성을 위한 이중화 |
| **Memory** | DDR4 ECC 256GB | 1 | etcd 성능 보장 |
| **Storage** | NVMe SSD 1TB (RAID 1) | 2 | etcd 데이터 |
| **Network** | 25GbE Dual Port | 2 | 본딩 구성 |
| **노드 수** | 3대 | - | HA 클러스터 (quorum) |

#### 2.2.2 Application Plane (애플리케이션 노드)

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | Intel Xeon Gold 6348 (28C/56T) | 2 | 컨테이너 워크로드 |
| **Memory** | DDR4 ECC 512GB | 1 | 다중 Pod 실행 |
| **Storage** | NVMe SSD 2TB | 2 | 로컬 캐시, 임시 저장 |
| **Network** | 25GbE Dual Port | 2 | 고대역폭 통신 |
| **노드 수** | 9대 이상 | - | 워크로드에 따라 확장 |

#### 2.2.3 Data Plane (데이터베이스 노드)

**PostgreSQL 클러스터**

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | Intel Xeon Gold 6348 (28C/56T) | 2 | 쿼리 처리 |
| **Memory** | DDR4 ECC 512GB | 1 | shared_buffers, 캐시 |
| **Storage** | NVMe SSD 4TB (RAID 10) | 4 | 고성능 I/O |
| **Network** | 25GbE Dual Port | 2 | 레플리케이션 트래픽 |
| **노드 수** | 3대 | - | Primary 1 + Replica 2 |

**Redis 클러스터**

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | Intel Xeon Silver 4314 (16C/32T) | 2 | 단순 연산 |
| **Memory** | DDR4 ECC 256GB | 1 | 인메모리 데이터 |
| **Storage** | NVMe SSD 1TB | 2 | AOF 지속성 |
| **Network** | 25GbE Dual Port | 2 | 클러스터 통신 |
| **노드 수** | 6대 | - | 3 Master + 3 Replica |

#### 2.2.4 Vector/Graph DB Plane (Milvus + Neo4j 클러스터)

**Milvus 클러스터**

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | Intel Xeon Gold 6348 (28C/56T) | 2 | 벡터 연산 |
| **Memory** | DDR4 ECC 512GB | 1 | 인덱스 캐시 |
| **Storage** | NVMe SSD 8TB | 4 | 벡터 데이터 |
| **Network** | 100GbE Dual Port | 2 | 대용량 벡터 전송 |
| **노드 수** | 6대 이상 | - | Query 2 + Data 2 + Index 1 + Coord 1 |

**Neo4j 클러스터 (GraphRAG)**

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | Intel Xeon Gold 6348 (28C/56T) | 2 | 그래프 쿼리 처리 |
| **Memory** | DDR4 ECC 256GB | 1 | 페이지 캐시 |
| **Storage** | NVMe SSD 2TB (RAID 10) | 4 | 그래프 데이터 |
| **Network** | 25GbE Dual Port | 2 | 클러스터 통신 |
| **노드 수** | 3대 | - | Core 3노드 (Causal Cluster) |

#### 2.2.5 GPU Plane (추론/학습 노드)

**추론 노드 (Inference)**

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | AMD EPYC 9654 (96C/192T) | 2 | GPU 피딩 |
| **Memory** | DDR5 ECC 1TB | 1 | 모델 로딩 |
| **GPU** | NVIDIA A100 80GB | 8 | NVLink 연결 |
| **Storage** | NVMe SSD 8TB | 4 | 모델 저장 |
| **Network** | 100GbE + InfiniBand HDR | 2+2 | GPU 통신 최적화 |
| **노드 수** | 4대 이상 | - | 동시 사용자에 따라 확장 |

**학습 노드 (Training)**

| 항목 | 사양 | 수량 | 비고 |
|------|------|------|------|
| **CPU** | AMD EPYC 9654 (96C/192T) | 2 | 데이터 전처리 |
| **Memory** | DDR5 ECC 2TB | 1 | 대용량 배치 |
| **GPU** | NVIDIA H100 80GB | 8 | NVLink + NVSwitch |
| **Storage** | NVMe SSD 16TB | 8 | 체크포인트 저장 |
| **Network** | InfiniBand NDR (400Gb/s) | 4 | 분산 학습 |
| **노드 수** | 4대 이상 | - | 파인튜닝 규모에 따라 |

### 2.3 스토리지 구성

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          스토리지 아키텍처                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                    Distributed Storage Layer                          │  │
│  │                                                                        │  │
│  │  ┌────────────────────────────────────────────────────────────────┐  │  │
│  │  │                    MinIO Cluster (Object Storage)               │  │  │
│  │  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐       │  │  │
│  │  │  │ MinIO-01 │  │ MinIO-02 │  │ MinIO-03 │  │ MinIO-04 │       │  │  │
│  │  │  │ 100TB    │  │ 100TB    │  │ 100TB    │  │ 100TB    │       │  │  │
│  │  │  └──────────┘  └──────────┘  └──────────┘  └──────────┘       │  │  │
│  │  │  Erasure Coding (EC:4) - 총 400TB, 실효 300TB                  │  │  │
│  │  └────────────────────────────────────────────────────────────────┘  │  │
│  │                                                                        │  │
│  │  용도: 모델 파일, 문서 원본, 파인튜닝 데이터셋, 백업                     │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                    Block Storage (Ceph/Longhorn)                      │  │
│  │                                                                        │  │
│  │  ┌─────────────────────┐  ┌─────────────────────┐                    │  │
│  │  │ PostgreSQL PV       │  │ Milvus PV           │                    │  │
│  │  │ 3-way Replication   │  │ 3-way Replication   │                    │  │
│  │  │ 총 12TB             │  │ 총 48TB             │                    │  │
│  │  └─────────────────────┘  └─────────────────────┘                    │  │
│  │                                                                        │  │
│  │  용도: 데이터베이스, 벡터 DB, 상태 저장 워크로드                        │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                    NFS/GPFS (Shared Storage)                          │  │
│  │                                                                        │  │
│  │  ┌─────────────────────┐  ┌─────────────────────┐                    │  │
│  │  │ Model Repository    │  │ Dataset Storage     │                    │  │
│  │  │ /models (100TB)     │  │ /datasets (200TB)   │                    │  │
│  │  └─────────────────────┘  └─────────────────────┘                    │  │
│  │                                                                        │  │
│  │  용도: GPU 노드 간 공유 모델, 학습 데이터                               │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.4 네트워크 구성

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          네트워크 토폴로지                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                        ┌───────────────────┐                                │
│                        │   Core Switch     │                                │
│                        │   (L3, 400GbE)    │                                │
│                        └─────────┬─────────┘                                │
│                                  │                                          │
│          ┌───────────────────────┼───────────────────────┐                  │
│          │                       │                       │                  │
│  ┌───────┴───────┐      ┌───────┴───────┐      ┌───────┴───────┐          │
│  │  Spine-01     │      │  Spine-02     │      │  Spine-03     │          │
│  │  (L3, 100GbE) │      │  (L3, 100GbE) │      │  (L3, 100GbE) │          │
│  └───────┬───────┘      └───────┬───────┘      └───────┬───────┘          │
│          │                       │                       │                  │
│          └───────────────────────┼───────────────────────┘                  │
│                                  │                                          │
│    ┌─────────────────────────────┼─────────────────────────────┐           │
│    │                             │                             │           │
│  ┌─┴───────────┐           ┌─────┴─────┐           ┌──────────┴─┐         │
│  │  Leaf-01    │           │  Leaf-02  │           │  Leaf-03   │         │
│  │  (25GbE)    │           │  (25GbE)  │           │  (100GbE)  │         │
│  └─┬───────────┘           └─────┬─────┘           └──────────┬─┘         │
│    │                             │                             │           │
│    ▼                             ▼                             ▼           │
│  Management &              Data Plane                    GPU Plane          │
│  Application               (DB, Vector)                  (InfiniBand)       │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     InfiniBand Fabric (GPU 전용)                     │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │   │
│  │  │ IB Switch-01│──│ IB Switch-02│──│ IB Switch-03│                  │   │
│  │  │ HDR 200Gb/s │  │ HDR 200Gb/s │  │ NDR 400Gb/s │                  │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │   │
│  │          │                │                │                         │   │
│  │     GPU 추론 노드    GPU 추론 노드    GPU 학습 노드                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 2.4.1 VLAN 구성

| VLAN ID | 이름 | 대역 | 용도 |
|---------|------|------|------|
| 10 | Management | 10.10.0.0/24 | 관리, SSH, IPMI |
| 20 | Kubernetes | 10.20.0.0/16 | Pod/Service 네트워크 |
| 30 | Storage | 10.30.0.0/24 | NFS, Ceph, MinIO |
| 40 | Database | 10.40.0.0/24 | PostgreSQL, Redis, Milvus |
| 50 | GPU-Ethernet | 10.50.0.0/24 | GPU 노드 이더넷 |
| 100 | External | 192.168.100.0/24 | 사용자 접근 |

---

## 3. Kubernetes 클러스터 구성

### 3.1 클러스터 사양

```yaml
# 클러스터 기본 정보
apiVersion: v1
kind: ClusterConfiguration
metadata:
  name: llmflow-cluster
spec:
  kubernetesVersion: "1.29.x"
  controlPlaneEndpoint: "k8s-api.llmflow.local:6443"
  networking:
    podSubnet: "10.20.0.0/16"
    serviceSubnet: "10.21.0.0/16"
    dnsDomain: "cluster.local"

  # CNI: Cilium (eBPF 기반)
  cni:
    type: cilium
    version: "1.15.x"
    config:
      enableNetworkPolicy: true
      enableBPFMasquerade: true
      enableHostReachableServices: true
      hubble:
        enabled: true
        relay:
          enabled: true
        ui:
          enabled: true
```

### 3.2 노드 레이블 및 Taint

```yaml
# GPU 노드 설정
apiVersion: v1
kind: Node
metadata:
  name: gpu-inference-01
  labels:
    node.kubernetes.io/instance-type: gpu-inference
    nvidia.com/gpu.product: A100-SXM4-80GB
    nvidia.com/gpu.count: "8"
    topology.kubernetes.io/zone: zone-a
    llmflow.io/workload-type: inference
spec:
  taints:
    - key: nvidia.com/gpu
      value: "true"
      effect: NoSchedule
    - key: llmflow.io/inference
      value: "true"
      effect: NoSchedule

---
# 학습 노드 설정
apiVersion: v1
kind: Node
metadata:
  name: gpu-training-01
  labels:
    node.kubernetes.io/instance-type: gpu-training
    nvidia.com/gpu.product: H100-SXM5-80GB
    nvidia.com/gpu.count: "8"
    llmflow.io/workload-type: training
spec:
  taints:
    - key: nvidia.com/gpu
      value: "true"
      effect: NoSchedule
    - key: llmflow.io/training
      value: "true"
      effect: NoSchedule
```

### 3.3 네임스페이스 구조

```yaml
# 시스템 네임스페이스
apiVersion: v1
kind: Namespace
metadata:
  name: llmflow-system
  labels:
    istio-injection: enabled
    pod-security.kubernetes.io/enforce: restricted

---
# 인프라 네임스페이스
apiVersion: v1
kind: Namespace
metadata:
  name: llmflow-infra
  labels:
    purpose: infrastructure

---
# 추론 네임스페이스
apiVersion: v1
kind: Namespace
metadata:
  name: llmflow-inference
  labels:
    purpose: model-inference
    gpu-required: "true"

---
# 테넌트 네임스페이스 템플릿
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-{tenant_id}
  labels:
    llmflow.io/tenant: "{tenant_id}"
    llmflow.io/department: "{department}"
    pod-security.kubernetes.io/enforce: baseline
  annotations:
    scheduler.alpha.kubernetes.io/node-selector: >
      llmflow.io/tenant-pool=shared
```

### 3.4 리소스 쿼터

```yaml
# 테넌트별 리소스 쿼터
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-quota
  namespace: tenant-{tenant_id}
spec:
  hard:
    # 컴퓨팅 리소스
    requests.cpu: "100"
    requests.memory: "200Gi"
    limits.cpu: "200"
    limits.memory: "400Gi"

    # GPU 리소스
    requests.nvidia.com/gpu: "4"
    limits.nvidia.com/gpu: "8"

    # 스토리지
    requests.storage: "500Gi"
    persistentvolumeclaims: "20"

    # 객체 수 제한
    pods: "100"
    services: "20"
    secrets: "50"
    configmaps: "50"

---
# LimitRange (Pod 기본값)
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-limits
  namespace: tenant-{tenant_id}
spec:
  limits:
    - type: Container
      default:
        cpu: "500m"
        memory: "512Mi"
      defaultRequest:
        cpu: "100m"
        memory: "128Mi"
      max:
        cpu: "8"
        memory: "32Gi"
      min:
        cpu: "50m"
        memory: "64Mi"
```

---

## 4. 배포 구성

### 4.1 Helm Chart 구조

```
llmflow-helm/
├── Chart.yaml
├── values.yaml
├── values-production.yaml
├── values-staging.yaml
├── charts/
│   ├── dify/
│   ├── milvus/
│   ├── neo4j/              # GraphRAG용 그래프 DB
│   ├── vllm/
│   ├── tei/                # Text Embeddings Inference (임베딩/리랭킹)
│   ├── unstructured/       # 문서 파싱 ETL/OCR
│   ├── llama-factory/      # 파인튜닝 WebUI
│   ├── keycloak/
│   ├── apisix/
│   ├── langfuse/
│   ├── mlflow/
│   └── monitoring/
└── templates/
    ├── _helpers.tpl
    ├── namespace.yaml
    ├── network-policies.yaml
    ├── resource-quotas.yaml
    └── secrets.yaml
```

### 4.2 메인 Values 파일

```yaml
# values-production.yaml
global:
  domain: llmflow.company.local
  storageClass: ceph-rbd
  imageRegistry: harbor.company.local
  imagePullSecrets:
    - name: harbor-credentials

# Dify 설정
dify:
  enabled: true
  replicaCount: 3
  image:
    repository: langgenius/dify
    tag: "0.11.0"

  api:
    replicas: 3
    resources:
      requests:
        cpu: "2"
        memory: "4Gi"
      limits:
        cpu: "4"
        memory: "8Gi"
    autoscaling:
      enabled: true
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilization: 70

  worker:
    replicas: 5
    resources:
      requests:
        cpu: "2"
        memory: "4Gi"
      limits:
        cpu: "4"
        memory: "8Gi"

  web:
    replicas: 3
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1"
        memory: "1Gi"

  config:
    secretKey: "${DIFY_SECRET_KEY}"
    deployment:
      editMode: false
    security:
      contentPolicy:
        enabled: true
      httpOnly: true

    # 외부 서비스 연결
    database:
      host: postgresql-primary.llmflow-infra
      port: 5432
      database: dify
      username: dify
      existingSecret: dify-db-credentials

    redis:
      host: redis-master.llmflow-infra
      port: 6379
      existingSecret: redis-credentials

    vectorStore:
      type: milvus
      host: milvus-proxy.llmflow-infra
      port: 19530

# vLLM 설정
vllm:
  enabled: true
  models:
    - name: llama-3.1-70b
      replicaCount: 2
      gpuCount: 4
      tensorParallelSize: 4
      maxModelLen: 32768
      resources:
        limits:
          nvidia.com/gpu: 4
          memory: "320Gi"
        requests:
          nvidia.com/gpu: 4
          memory: "280Gi"

    - name: llama-3.1-8b
      replicaCount: 4
      gpuCount: 1
      tensorParallelSize: 1
      maxModelLen: 32768
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: "80Gi"
        requests:
          nvidia.com/gpu: 1
          memory: "60Gi"

    - name: qwen-2.5-72b
      replicaCount: 2
      gpuCount: 4
      tensorParallelSize: 4
      maxModelLen: 131072
      resources:
        limits:
          nvidia.com/gpu: 4
          memory: "320Gi"
        requests:
          nvidia.com/gpu: 4
          memory: "280Gi"

  config:
    maxConcurrentRequests: 256
    gpuMemoryUtilization: 0.9
    enablePrefixCaching: true
    enableChunkedPrefill: true

# TEI (Text Embeddings Inference) 설정
tei:
  enabled: true

  embedding:
    replicaCount: 3
    image:
      repository: ghcr.io/huggingface/text-embeddings-inference
      tag: "1.5"
    model: BAAI/bge-m3
    resources:
      requests:
        cpu: "4"
        memory: "8Gi"
      limits:
        cpu: "8"
        memory: "16Gi"
    autoscaling:
      enabled: true
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilization: 70

  reranker:
    replicaCount: 2
    image:
      repository: ghcr.io/huggingface/text-embeddings-inference
      tag: "1.5"
    model: BAAI/bge-reranker-v2-m3
    resources:
      requests:
        cpu: "4"
        memory: "8Gi"
      limits:
        cpu: "8"
        memory: "16Gi"

# Unstructured 설정 (문서 파싱/ETL)
unstructured:
  enabled: true
  replicaCount: 3
  image:
    repository: quay.io/unstructured-io/unstructured-api
    tag: "0.0.80"
  resources:
    requests:
      cpu: "2"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "8Gi"
  config:
    enableOCR: true
    ocrLanguages: ["eng", "kor"]
    chunkingStrategy: "by_title"
    maxCharacters: 1500
    newAfterNChars: 1000
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilization: 70

# Neo4j 설정 (GraphRAG)
neo4j:
  enabled: true
  mode: cluster  # standalone 또는 cluster

  core:
    replicas: 3
    resources:
      requests:
        cpu: "4"
        memory: "16Gi"
      limits:
        cpu: "8"
        memory: "32Gi"
    persistence:
      size: 500Gi
      storageClass: ceph-rbd

  config:
    dbms.memory.heap.initial_size: "8G"
    dbms.memory.heap.max_size: "16G"
    dbms.memory.pagecache.size: "8G"
    dbms.security.auth_enabled: "true"

  plugins:
    - apoc
    - graph-data-science

# LLaMA-Factory 설정 (파인튜닝)
llamaFactory:
  enabled: true
  replicaCount: 1
  image:
    repository: hiyouga/llama-factory
    tag: "latest"
  resources:
    requests:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: 1
    limits:
      cpu: "16"
      memory: "64Gi"
      nvidia.com/gpu: 1
  config:
    webui:
      port: 7860
    deepspeed:
      enabled: true
      stage: 2  # ZeRO Stage 2
    methods:
      - lora
      - qlora
      - dora
      - full
  storage:
    models:
      size: 500Gi
      storageClass: nfs-client
    datasets:
      size: 200Gi
      storageClass: nfs-client
    outputs:
      size: 500Gi
      storageClass: nfs-client
  nodeSelector:
    llmflow.io/workload-type: training
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

# Milvus 설정
milvus:
  enabled: true
  mode: cluster

  proxy:
    replicas: 2
    resources:
      requests:
        cpu: "2"
        memory: "4Gi"
      limits:
        cpu: "4"
        memory: "8Gi"

  queryNode:
    replicas: 3
    resources:
      requests:
        cpu: "4"
        memory: "32Gi"
      limits:
        cpu: "8"
        memory: "64Gi"

  dataNode:
    replicas: 2
    resources:
      requests:
        cpu: "2"
        memory: "16Gi"
      limits:
        cpu: "4"
        memory: "32Gi"

  indexNode:
    replicas: 2
    resources:
      requests:
        cpu: "4"
        memory: "32Gi"
      limits:
        cpu: "8"
        memory: "64Gi"

  storage:
    type: minio
    endpoint: minio.llmflow-infra:9000
    bucketName: milvus
    accessKey: "${MINIO_ACCESS_KEY}"
    secretKey: "${MINIO_SECRET_KEY}"

# Keycloak 설정
keycloak:
  enabled: true
  replicas: 3

  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

  postgresql:
    enabled: false
    external:
      host: postgresql-primary.llmflow-infra
      port: 5432
      database: keycloak
      existingSecret: keycloak-db-credentials

  config:
    hostname: auth.llmflow.company.local
    https:
      enabled: true
      certificateSecret: keycloak-tls

    # LDAP 페더레이션
    ldap:
      enabled: true
      vendor: ad
      connectionUrl: ldaps://ldap.company.local:636
      usersDn: "OU=Users,DC=company,DC=local"
      bindDn: "CN=svc-keycloak,OU=ServiceAccounts,DC=company,DC=local"
      bindCredential: "${LDAP_BIND_PASSWORD}"

# APISIX 설정
apisix:
  enabled: true
  replicas: 3

  resources:
    requests:
      cpu: "2"
      memory: "2Gi"
    limits:
      cpu: "4"
      memory: "4Gi"

  config:
    enableIPv6: false
    ssl:
      enabled: true
      certSecret: apisix-tls

    plugins:
      - jwt-auth
      - key-auth
      - limit-req
      - limit-count
      - prometheus
      - opentelemetry
      - ai-proxy
      - ai-rate-limiting

# Langfuse 설정
langfuse:
  enabled: true
  replicas: 2

  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

  postgresql:
    external:
      host: postgresql-primary.llmflow-infra
      port: 5432
      database: langfuse
      existingSecret: langfuse-db-credentials

# MLflow 설정
mlflow:
  enabled: true
  replicas: 2

  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

  artifactStore:
    type: s3
    endpoint: minio.llmflow-infra:9000
    bucket: mlflow-artifacts

  backendStore:
    type: postgresql
    host: postgresql-primary.llmflow-infra
    port: 5432
    database: mlflow

# 모니터링 스택
monitoring:
  enabled: true

  prometheus:
    replicas: 2
    retention: 30d
    storageSize: 500Gi

    alertmanager:
      replicas: 3

  grafana:
    replicas: 2
    dashboards:
      - llmflow-overview
      - gpu-metrics
      - inference-latency
      - rag-performance

  loki:
    enabled: true
    retention: 14d
    storageSize: 200Gi
```

### 4.3 Docker Compose (개발/테스트 환경)

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ============ Core Services ============
  dify-api:
    image: langgenius/dify-api:0.11.0
    restart: always
    environment:
      MODE: api
      LOG_LEVEL: INFO
      SECRET_KEY: ${DIFY_SECRET_KEY}

      # Database
      DB_HOST: postgresql
      DB_PORT: 5432
      DB_DATABASE: dify
      DB_USERNAME: dify
      DB_PASSWORD: ${DB_PASSWORD}

      # Redis
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}

      # Vector Store
      VECTOR_STORE: milvus
      MILVUS_HOST: milvus
      MILVUS_PORT: 19530

      # Storage
      STORAGE_TYPE: s3
      S3_ENDPOINT: http://minio:9000
      S3_BUCKET_NAME: dify
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY}
    depends_on:
      - postgresql
      - redis
      - milvus
      - minio
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G

  dify-worker:
    image: langgenius/dify-api:0.11.0
    restart: always
    environment:
      MODE: worker
      LOG_LEVEL: INFO
      SECRET_KEY: ${DIFY_SECRET_KEY}
      DB_HOST: postgresql
      DB_PORT: 5432
      DB_DATABASE: dify
      DB_USERNAME: dify
      DB_PASSWORD: ${DB_PASSWORD}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    depends_on:
      - postgresql
      - redis
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G

  dify-web:
    image: langgenius/dify-web:0.11.0
    restart: always
    environment:
      CONSOLE_API_URL: http://dify-api:5001
      APP_API_URL: http://dify-api:5001
    depends_on:
      - dify-api
    networks:
      - llmflow-net

  # ============ Inference ============
  vllm:
    image: vllm/vllm-openai:v0.6.0
    restart: always
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      HF_HOME: /models
    command: >
      --model /models/Meta-Llama-3.1-8B-Instruct
      --tensor-parallel-size 1
      --max-model-len 32768
      --gpu-memory-utilization 0.9
      --enable-prefix-caching
    volumes:
      - model-storage:/models
    networks:
      - llmflow-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============ Embedding & Reranking (TEI) ============
  tei-embedding:
    image: ghcr.io/huggingface/text-embeddings-inference:1.5
    restart: always
    environment:
      MODEL_ID: BAAI/bge-m3
      MAX_CLIENT_BATCH_SIZE: 32
      MAX_BATCH_TOKENS: 16384
    volumes:
      - tei-models:/data
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G

  tei-reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:1.5
    restart: always
    environment:
      MODEL_ID: BAAI/bge-reranker-v2-m3
      MAX_CLIENT_BATCH_SIZE: 32
    volumes:
      - tei-models:/data
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G

  # ============ Document Parsing (Unstructured) ============
  unstructured:
    image: quay.io/unstructured-io/unstructured-api:0.0.80
    restart: always
    environment:
      UNSTRUCTURED_PARALLEL_MODE_ENABLED: "true"
      UNSTRUCTURED_PARALLEL_MODE_THREADS: "4"
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G

  # ============ Graph DB (Neo4j) ============
  neo4j:
    image: neo4j:5.15-enterprise
    restart: always
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD}
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_dbms_memory_heap_initial__size: "2G"
      NEO4J_dbms_memory_heap_max__size: "4G"
      NEO4J_dbms_memory_pagecache_size: "2G"
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    ports:
      - "7474:7474"   # HTTP
      - "7687:7687"   # Bolt
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G

  # ============ Fine-tuning (LLaMA-Factory) ============
  llama-factory:
    image: hiyouga/llama-factory:latest
    restart: always
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      GRADIO_SERVER_NAME: "0.0.0.0"
      GRADIO_SERVER_PORT: "7860"
    command: llamafactory-cli webui
    volumes:
      - llama-factory-models:/app/models
      - llama-factory-data:/app/data
      - llama-factory-output:/app/output
    ports:
      - "7860:7860"
    networks:
      - llmflow-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ============ Vector DB ============
  milvus:
    image: milvusdb/milvus:v2.4.0
    restart: always
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    command: ["milvus", "run", "standalone"]
    volumes:
      - milvus-data:/var/lib/milvus
    depends_on:
      - etcd
      - minio
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G

  etcd:
    image: quay.io/coreos/etcd:v3.5.11
    restart: always
    environment:
      ETCD_AUTO_COMPACTION_MODE: revision
      ETCD_AUTO_COMPACTION_RETENTION: "1000"
      ETCD_QUOTA_BACKEND_BYTES: "4294967296"
    command: >
      etcd
      --advertise-client-urls=http://127.0.0.1:2379
      --listen-client-urls=http://0.0.0.0:2379
    volumes:
      - etcd-data:/etcd
    networks:
      - llmflow-net

  # ============ Database ============
  postgresql:
    image: postgres:16-alpine
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_MULTIPLE_DATABASES: dify,keycloak,langfuse,mlflow
    volumes:
      - postgresql-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - llmflow-net
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G

  redis:
    image: redis:7-alpine
    restart: always
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    networks:
      - llmflow-net

  # ============ Storage ============
  minio:
    image: minio/minio:RELEASE.2024-01-01T00-00-00Z
    restart: always
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - llmflow-net

  # ============ Auth ============
  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    restart: always
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgresql:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD}
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_HOSTNAME: auth.llmflow.local
      KC_PROXY: edge
    command: start --optimized
    depends_on:
      - postgresql
    networks:
      - llmflow-net

  # ============ API Gateway ============
  apisix:
    image: apache/apisix:3.8.0-debian
    restart: always
    volumes:
      - ./apisix/config.yaml:/usr/local/apisix/conf/config.yaml
      - ./apisix/apisix.yaml:/usr/local/apisix/conf/apisix.yaml
    ports:
      - "9080:9080"
      - "9443:9443"
    depends_on:
      - etcd
    networks:
      - llmflow-net

  # ============ Observability ============
  langfuse:
    image: langfuse/langfuse:2
    restart: always
    environment:
      DATABASE_URL: postgresql://langfuse:${LANGFUSE_DB_PASSWORD}@postgresql:5432/langfuse
      NEXTAUTH_URL: http://langfuse.llmflow.local
      NEXTAUTH_SECRET: ${LANGFUSE_SECRET}
      SALT: ${LANGFUSE_SALT}
    depends_on:
      - postgresql
    networks:
      - llmflow-net

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    restart: always
    environment:
      MLFLOW_BACKEND_STORE_URI: postgresql://mlflow:${MLFLOW_DB_PASSWORD}@postgresql:5432/mlflow
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://mlflow-artifacts
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    command: server --host 0.0.0.0 --port 5000
    depends_on:
      - postgresql
      - minio
    networks:
      - llmflow-net

  # ============ Monitoring ============
  prometheus:
    image: prom/prometheus:v2.48.0
    restart: always
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - llmflow-net

  grafana:
    image: grafana/grafana:10.2.3
    restart: always
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    networks:
      - llmflow-net

volumes:
  postgresql-data:
  redis-data:
  minio-data:
  milvus-data:
  etcd-data:
  model-storage:
  prometheus-data:
  grafana-data:
  tei-models:
  neo4j-data:
  neo4j-logs:
  llama-factory-models:
  llama-factory-data:
  llama-factory-output:

networks:
  llmflow-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
```

---

## 5. 고가용성 (HA) 구성

### 5.1 HA 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         고가용성 아키텍처                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────┐ │
│  │                      Load Balancer (L4/L7)                            │ │
│  │  ┌─────────────────┐              ┌─────────────────┐                 │ │
│  │  │   HAProxy-01    │◄────────────►│   HAProxy-02    │                 │ │
│  │  │   (Active)      │   Keepalived │   (Standby)     │                 │ │
│  │  └────────┬────────┘   VIP Float  └────────┬────────┘                 │ │
│  │           │                                 │                          │ │
│  │           └─────────────┬───────────────────┘                          │ │
│  │                         │ VIP: 192.168.100.100                         │ │
│  └─────────────────────────┼─────────────────────────────────────────────┘ │
│                            │                                               │
│  ┌─────────────────────────┼─────────────────────────────────────────────┐ │
│  │                   API Gateway Cluster                                  │ │
│  │     ┌─────────┐    ┌─────────┐    ┌─────────┐                         │ │
│  │     │APISIX-01│    │APISIX-02│    │APISIX-03│                         │ │
│  │     └────┬────┘    └────┬────┘    └────┬────┘                         │ │
│  │          └──────────────┼──────────────┘                               │ │
│  └─────────────────────────┼─────────────────────────────────────────────┘ │
│                            │                                               │
│  ┌─────────────────────────┼─────────────────────────────────────────────┐ │
│  │                   Application Cluster                                  │ │
│  │     ┌─────────┐    ┌─────────┐    ┌─────────┐                         │ │
│  │     │ Dify-01 │    │ Dify-02 │    │ Dify-03 │                         │ │
│  │     └────┬────┘    └────┬────┘    └────┬────┘                         │ │
│  │          │              │              │                               │ │
│  │     ┌─────────┐    ┌─────────┐    ┌─────────┐                         │ │
│  │     │Worker-01│    │Worker-02│    │Worker-03│                         │ │
│  │     └─────────┘    └─────────┘    └─────────┘                         │ │
│  └─────────────────────────┼─────────────────────────────────────────────┘ │
│                            │                                               │
│  ┌─────────────────────────┼─────────────────────────────────────────────┐ │
│  │                   Database Cluster                                     │ │
│  │                                                                        │ │
│  │  PostgreSQL (Patroni HA)           Redis Sentinel                     │ │
│  │  ┌──────────────────────┐          ┌──────────────────────┐           │ │
│  │  │   ┌─────┐            │          │  ┌────────┐          │           │ │
│  │  │   │ PG  │◄──Repl────►│          │  │Sentinel│ x3       │           │ │
│  │  │   │Pri  │            │          │  └────────┘          │           │ │
│  │  │   └──┬──┘            │          │       │              │           │ │
│  │  │      │               │          │  ┌────┴────┐         │           │ │
│  │  │  ┌───┴───┐           │          │  │         │         │           │ │
│  │  │  ▼       ▼           │          │  ▼         ▼         │           │ │
│  │  │┌─────┐ ┌─────┐       │          │┌──────┐ ┌──────┐     │           │ │
│  │  ││ PG  │ │ PG  │       │          ││Redis │ │Redis │     │           │ │
│  │  ││Rep1 │ │Rep2 │       │          ││Master│ │Replica│    │           │ │
│  │  │└─────┘ └─────┘       │          │└──────┘ └──────┘     │           │ │
│  │  └──────────────────────┘          └──────────────────────┘           │ │
│  └───────────────────────────────────────────────────────────────────────┘ │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 PostgreSQL HA (Patroni)

```yaml
# patroni.yml
scope: llmflow-postgres
namespace: /service/
name: postgresql-01

restapi:
  listen: 0.0.0.0:8008
  connect_address: postgresql-01:8008

etcd3:
  hosts:
    - etcd-01:2379
    - etcd-02:2379
    - etcd-03:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        max_connections: 500
        shared_buffers: 32GB
        effective_cache_size: 96GB
        maintenance_work_mem: 2GB
        checkpoint_completion_target: 0.9
        wal_buffers: 64MB
        default_statistics_target: 100
        random_page_cost: 1.1
        effective_io_concurrency: 200
        work_mem: 64MB
        min_wal_size: 2GB
        max_wal_size: 8GB
        max_worker_processes: 8
        max_parallel_workers_per_gather: 4
        max_parallel_workers: 8
        max_parallel_maintenance_workers: 4
        wal_level: replica
        hot_standby: "on"
        max_wal_senders: 10
        max_replication_slots: 10
        hot_standby_feedback: "on"

  initdb:
    - encoding: UTF8
    - data-checksums

  pg_hba:
    - host replication replicator 10.40.0.0/24 scram-sha-256
    - host all all 10.20.0.0/16 scram-sha-256
    - host all all 10.40.0.0/24 scram-sha-256

postgresql:
  listen: 0.0.0.0:5432
  connect_address: postgresql-01:5432
  data_dir: /var/lib/postgresql/data
  pgpass: /tmp/pgpass0
  authentication:
    replication:
      username: replicator
      password: ${REPLICATOR_PASSWORD}
    superuser:
      username: postgres
      password: ${POSTGRES_PASSWORD}
    rewind:
      username: rewind_user
      password: ${REWIND_PASSWORD}
```

### 5.3 Redis Sentinel 구성

```yaml
# redis-sentinel.conf
port 26379
sentinel monitor llmflow-redis redis-master 6379 2
sentinel auth-pass llmflow-redis ${REDIS_PASSWORD}
sentinel down-after-milliseconds llmflow-redis 5000
sentinel failover-timeout llmflow-redis 60000
sentinel parallel-syncs llmflow-redis 1

# Redis Master 설정
# redis-master.conf
port 6379
requirepass ${REDIS_PASSWORD}
masterauth ${REDIS_PASSWORD}

# 복제 설정
replica-serve-stale-data yes
replica-read-only yes
repl-diskless-sync yes
repl-diskless-sync-delay 5

# 지속성
appendonly yes
appendfsync everysec
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# 메모리 관리
maxmemory 64gb
maxmemory-policy volatile-lru

# 클러스터 설정 (선택적)
# cluster-enabled yes
# cluster-config-file nodes.conf
# cluster-node-timeout 15000
```

### 5.4 Milvus HA 구성

```yaml
# milvus-cluster.yaml
apiVersion: milvus.io/v1beta1
kind: Milvus
metadata:
  name: llmflow-milvus
  namespace: llmflow-infra
spec:
  mode: cluster

  dependencies:
    etcd:
      inCluster:
        values:
          replicaCount: 3
          persistence:
            enabled: true
            storageClass: ceph-rbd
            size: 50Gi

    storage:
      inCluster:
        values:
          mode: distributed
          replicas: 4
          persistence:
            enabled: true
            storageClass: ceph-rbd
            size: 500Gi

    pulsar:
      inCluster:
        values:
          components:
            autorecovery: true
          broker:
            replicaCount: 3
          bookkeeper:
            replicaCount: 3
          zookeeper:
            replicaCount: 3

  components:
    proxy:
      replicas: 3
      resources:
        requests:
          cpu: "2"
          memory: "4Gi"
        limits:
          cpu: "4"
          memory: "8Gi"

    rootCoord:
      replicas: 2
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"

    queryCoord:
      replicas: 2
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"

    queryNode:
      replicas: 4
      resources:
        requests:
          cpu: "4"
          memory: "32Gi"

    dataCoord:
      replicas: 2
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"

    dataNode:
      replicas: 3
      resources:
        requests:
          cpu: "2"
          memory: "16Gi"

    indexCoord:
      replicas: 2
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"

    indexNode:
      replicas: 3
      resources:
        requests:
          cpu: "4"
          memory: "32Gi"
```

---

## 6. 확장성 전략

### 6.1 수평 확장 (Horizontal Scaling)

```yaml
# HPA 설정 - Dify API
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dify-api-hpa
  namespace: llmflow-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dify-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max

---
# KEDA ScaledObject - 큐 기반 스케일링
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: dify-worker-scaler
  namespace: llmflow-system
spec:
  scaleTargetRef:
    name: dify-worker
  minReplicaCount: 3
  maxReplicaCount: 30
  pollingInterval: 15
  cooldownPeriod: 300
  triggers:
    - type: redis
      metadata:
        address: redis-master.llmflow-infra:6379
        listName: celery
        listLength: "50"
        enableTLS: "false"
      authenticationRef:
        name: redis-auth
```

### 6.2 GPU 노드 자동 확장

```yaml
# GPU Node Pool 자동 확장 (Cluster Autoscaler 연동)
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  config.yaml: |
    node-groups:
      - name: gpu-inference-pool
        minSize: 4
        maxSize: 16
        nodeSelector:
          llmflow.io/workload-type: inference
        scaleDown:
          enabled: true
          delayAfterAdd: 10m
          delayAfterDelete: 0s
          unneededTime: 10m
          utilizationThreshold: 0.5

      - name: gpu-training-pool
        minSize: 2
        maxSize: 8
        nodeSelector:
          llmflow.io/workload-type: training
        scaleDown:
          enabled: true
          delayAfterAdd: 30m
          unneededTime: 30m
          utilizationThreshold: 0.3
```

### 6.3 vLLM 자동 확장 (Kubernetes 네이티브)

vLLM은 Ray 없이 단독으로 실행하며, Kubernetes HPA와 KEDA를 사용하여 자동 확장합니다.

```yaml
# vLLM Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama-70b
  namespace: llmflow-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm
      model: llama-70b
  template:
    metadata:
      labels:
        app: vllm
        model: llama-70b
    spec:
      nodeSelector:
        llmflow.io/workload-type: inference
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          args:
            - --model=/models/Meta-Llama-3.1-70B-Instruct
            - --tensor-parallel-size=4
            - --max-model-len=32768
            - --gpu-memory-utilization=0.9
            - --enable-prefix-caching
            - --enable-chunked-prefill
          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8080
              name: metrics
          resources:
            limits:
              nvidia.com/gpu: "4"
              memory: "320Gi"
            requests:
              nvidia.com/gpu: "4"
              memory: "280Gi"
          volumeMounts:
            - name: model-storage
              mountPath: /models
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-storage-pvc

---
# vLLM HPA (Request 기반 스케일링)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-llama-70b-hpa
  namespace: llmflow-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-llama-70b
  minReplicas: 2
  maxReplicas: 8
  metrics:
    # GPU 메모리 활용률
    - type: Pods
      pods:
        metric:
          name: vllm_gpu_cache_usage_perc
        target:
          type: AverageValue
          averageValue: "80"
    # 진행 중인 요청 수
    - type: Pods
      pods:
        metric:
          name: vllm_num_requests_running
        target:
          type: AverageValue
          averageValue: "10"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # GPU 워크로드는 보수적으로
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60

---
# KEDA ScaledObject (외부 메트릭 기반)
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: vllm-llama-70b-keda
  namespace: llmflow-inference
spec:
  scaleTargetRef:
    name: vllm-llama-70b
  minReplicaCount: 2
  maxReplicaCount: 8
  pollingInterval: 15
  cooldownPeriod: 600
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.llmflow-system:9090
        metricName: vllm_pending_requests
        query: sum(vllm_num_requests_waiting{model="llama-70b"})
        threshold: "20"
```

### 6.4 TEI (임베딩/리랭킹) 자동 확장

```yaml
# TEI Embedding HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tei-embedding-hpa
  namespace: llmflow-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tei-embedding
  minReplicas: 3
  maxReplicas: 15
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: tei_request_duration_seconds
        target:
          type: AverageValue
          averageValue: "100m"  # 100ms

---
# TEI Reranker HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tei-reranker-hpa
  namespace: llmflow-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tei-reranker
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

### 6.5 Unstructured (문서 파싱) 자동 확장

```yaml
# Unstructured HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: unstructured-hpa
  namespace: llmflow-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: unstructured
  minReplicas: 3
  maxReplicas: 15
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
```

---

## 7. 모니터링 및 운영

### 7.1 메트릭 수집 구성

```yaml
# ServiceMonitor for Dify
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dify-metrics
  namespace: llmflow-system
spec:
  selector:
    matchLabels:
      app: dify
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics

---
# ServiceMonitor for vLLM
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-metrics
  namespace: llmflow-inference
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
    - port: metrics
      interval: 10s
      path: /metrics

---
# PodMonitor for GPU Metrics
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: gpu-metrics
  namespace: llmflow-inference
spec:
  selector:
    matchLabels:
      gpu-monitoring: enabled
  podMetricsEndpoints:
    - port: nvidia-dcgm
      interval: 10s

---
# ServiceMonitor for TEI Embedding
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tei-embedding-metrics
  namespace: llmflow-inference
spec:
  selector:
    matchLabels:
      app: tei-embedding
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics

---
# ServiceMonitor for TEI Reranker
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tei-reranker-metrics
  namespace: llmflow-inference
spec:
  selector:
    matchLabels:
      app: tei-reranker
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics

---
# ServiceMonitor for Neo4j
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: neo4j-metrics
  namespace: llmflow-infra
spec:
  selector:
    matchLabels:
      app: neo4j
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics

---
# ServiceMonitor for Unstructured
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: unstructured-metrics
  namespace: llmflow-system
spec:
  selector:
    matchLabels:
      app: unstructured
  endpoints:
    - port: http
      interval: 30s
      path: /healthcheck
```

### 7.2 알림 규칙

```yaml
# PrometheusRule
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: llmflow-alerts
  namespace: llmflow-system
spec:
  groups:
    - name: llmflow.inference
      interval: 30s
      rules:
        - alert: HighInferenceLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(vllm_request_latency_seconds_bucket[5m])) by (le, model)
            ) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High inference latency detected"
            description: "P95 latency for {{ $labels.model }} is {{ $value }}s"

        - alert: GPUMemoryHigh
          expr: |
            DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL * 100 > 95
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "GPU memory usage critical"
            description: "GPU {{ $labels.gpu }} memory usage is {{ $value }}%"

        - alert: ModelLoadFailure
          expr: |
            increase(vllm_model_load_failures_total[5m]) > 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Model loading failed"
            description: "Model {{ $labels.model }} failed to load"

    - name: llmflow.infrastructure
      interval: 30s
      rules:
        - alert: PostgreSQLReplicationLag
          expr: |
            pg_replication_lag_seconds > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PostgreSQL replication lag"
            description: "Replication lag is {{ $value }}s on {{ $labels.instance }}"

        - alert: MilvusQueryLatencyHigh
          expr: |
            histogram_quantile(0.95,
              sum(rate(milvus_search_latency_bucket[5m])) by (le)
            ) > 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Milvus query latency high"
            description: "P95 search latency is {{ $value }}s"

        - alert: RedisMemoryHigh
          expr: |
            redis_memory_used_bytes / redis_memory_max_bytes * 100 > 85
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Redis memory usage high"
            description: "Redis memory usage is {{ $value }}%"

    - name: llmflow.embedding
      interval: 30s
      rules:
        - alert: TEIEmbeddingLatencyHigh
          expr: |
            histogram_quantile(0.95,
              sum(rate(tei_request_duration_seconds_bucket{service="embedding"}[5m])) by (le)
            ) > 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "TEI embedding latency high"
            description: "P95 embedding latency is {{ $value }}s"

        - alert: TEIRerankerLatencyHigh
          expr: |
            histogram_quantile(0.95,
              sum(rate(tei_request_duration_seconds_bucket{service="reranker"}[5m])) by (le)
            ) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "TEI reranker latency high"
            description: "P95 reranker latency is {{ $value }}s"

    - name: llmflow.graphdb
      interval: 30s
      rules:
        - alert: Neo4jClusterNotHealthy
          expr: |
            neo4j_cluster_core_is_leader == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Neo4j cluster has no leader"
            description: "Neo4j cluster leader election failed"

        - alert: Neo4jQueryLatencyHigh
          expr: |
            histogram_quantile(0.95,
              sum(rate(neo4j_bolt_messages_done_bucket[5m])) by (le)
            ) > 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Neo4j query latency high"
            description: "P95 query latency is {{ $value }}s"

    - name: llmflow.documentprocessing
      interval: 30s
      rules:
        - alert: UnstructuredProcessingQueueHigh
          expr: |
            unstructured_pending_documents > 100
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Unstructured processing queue high"
            description: "{{ $value }} documents waiting to be processed"

        - alert: UnstructuredOCRFailureRate
          expr: |
            rate(unstructured_ocr_failures_total[5m]) /
            rate(unstructured_ocr_requests_total[5m]) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High OCR failure rate"
            description: "OCR failure rate is {{ $value | humanizePercentage }}"
```

### 7.3 Grafana 대시보드

```json
{
  "dashboard": {
    "title": "LLMFlow Overview v2.0",
    "panels": [
      {
        "title": "Active Users",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(dify_active_sessions)",
            "legendFormat": "Active Users"
          }
        ]
      },
      {
        "title": "Inference Requests/sec",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(vllm_requests_total[1m])) by (model)",
            "legendFormat": "{{ model }}"
          }
        ]
      },
      {
        "title": "P95 Latency by Model",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(vllm_request_latency_seconds_bucket[5m])) by (le, model))",
            "legendFormat": "{{ model }}"
          }
        ]
      },
      {
        "title": "GPU Utilization",
        "type": "heatmap",
        "targets": [
          {
            "expr": "DCGM_FI_DEV_GPU_UTIL",
            "legendFormat": "{{ gpu }}"
          }
        ]
      },
      {
        "title": "Token Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(vllm_generation_tokens_total[1m]))",
            "legendFormat": "Tokens/sec"
          }
        ]
      },
      {
        "title": "Vector Search Latency (Milvus)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(milvus_search_latency_bucket[5m])) by (le))",
            "legendFormat": "P95 Latency"
          }
        ]
      },
      {
        "title": "TEI Embedding Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(tei_request_duration_seconds_bucket{service='embedding'}[5m])) by (le))",
            "legendFormat": "P95 Embedding"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(tei_request_duration_seconds_bucket{service='reranker'}[5m])) by (le))",
            "legendFormat": "P95 Reranking"
          }
        ]
      },
      {
        "title": "TEI Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(tei_requests_total{service='embedding'}[1m]))",
            "legendFormat": "Embedding req/s"
          },
          {
            "expr": "sum(rate(tei_requests_total{service='reranker'}[1m]))",
            "legendFormat": "Reranking req/s"
          }
        ]
      },
      {
        "title": "Neo4j Graph Queries",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(neo4j_bolt_messages_done[1m]))",
            "legendFormat": "Queries/sec"
          }
        ]
      },
      {
        "title": "Neo4j Query Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(neo4j_bolt_messages_done_bucket[5m])) by (le))",
            "legendFormat": "P95 Latency"
          }
        ]
      },
      {
        "title": "Document Processing (Unstructured)",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(unstructured_documents_processed_total[1m]))",
            "legendFormat": "Docs/sec"
          },
          {
            "expr": "sum(unstructured_pending_documents)",
            "legendFormat": "Pending"
          }
        ]
      },
      {
        "title": "Hybrid RAG Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(rag_vector_search_latency_bucket[5m])) by (le))",
            "legendFormat": "Vector Search P95"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(rag_graph_search_latency_bucket[5m])) by (le))",
            "legendFormat": "Graph Search P95"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(rag_fusion_latency_bucket[5m])) by (le))",
            "legendFormat": "RRF Fusion P95"
          }
        ]
      }
    ]
  }
}
```

### 7.4 로그 수집 구성

```yaml
# Fluent Bit ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: llmflow-system
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    [INPUT]
        Name              tail
        Tag               kube.*
        Path              /var/log/containers/*llmflow*.log
        Parser            docker
        DB                /var/log/flb_kube.db
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  10

    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On

    [FILTER]
        Name          modify
        Match         kube.*
        Add           cluster llmflow-production
        Add           environment production

    [OUTPUT]
        Name            opensearch
        Match           kube.*
        Host            opensearch.llmflow-infra
        Port            9200
        Index           llmflow-logs
        Type            _doc
        HTTP_User       fluent
        HTTP_Passwd     ${OPENSEARCH_PASSWORD}
        tls             On
        tls.verify      Off
        Suppress_Type_Name On
        Trace_Error     On
        Replace_Dots    On
```

---

## 8. 백업 및 복구

### 8.1 백업 전략

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          백업 전략 개요                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     데이터 분류별 백업 정책                           │   │
│  │                                                                       │   │
│  │  ┌─────────────┬─────────────┬─────────────┬─────────────────────┐  │   │
│  │  │   데이터     │   백업주기   │   보관기간   │      백업방식        │  │   │
│  │  ├─────────────┼─────────────┼─────────────┼─────────────────────┤  │   │
│  │  │ PostgreSQL  │ 매일 Full   │ 30일        │ pg_dump + WAL      │  │   │
│  │  │             │ 매시간 WAL  │ 7일         │ Continuous Archive │  │   │
│  │  ├─────────────┼─────────────┼─────────────┼─────────────────────┤  │   │
│  │  │ Milvus      │ 매일        │ 14일        │ Snapshot           │  │   │
│  │  │ Collections │             │             │                     │  │   │
│  │  ├─────────────┼─────────────┼─────────────┼─────────────────────┤  │   │
│  │  │ MinIO       │ 매주 Full   │ 90일        │ mc mirror          │  │   │
│  │  │ (Models)    │ 매일 Incr   │ 30일        │ + versioning       │  │   │
│  │  ├─────────────┼─────────────┼─────────────┼─────────────────────┤  │   │
│  │  │ etcd        │ 매 4시간    │ 7일         │ etcdctl snapshot    │  │   │
│  │  │ (K8s)       │             │             │                     │  │   │
│  │  ├─────────────┼─────────────┼─────────────┼─────────────────────┤  │   │
│  │  │ 설정 파일   │ Git 기반    │ 무제한      │ GitOps             │  │   │
│  │  │ (ConfigMaps)│             │             │                     │  │   │
│  │  └─────────────┴─────────────┴─────────────┴─────────────────────┘  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     백업 스토리지 아키텍처                            │   │
│  │                                                                       │   │
│  │     Primary Site                    Backup Site                      │   │
│  │   ┌─────────────────┐            ┌─────────────────┐                │   │
│  │   │   Production    │            │   DR Storage    │                │   │
│  │   │   Databases     │───────────►│   (Offsite)     │                │   │
│  │   └─────────────────┘  Async     └─────────────────┘                │   │
│  │                       Replication                                    │   │
│  │   ┌─────────────────┐            ┌─────────────────┐                │   │
│  │   │   Local Backup  │───────────►│   Cold Storage  │                │   │
│  │   │   (NFS/SAN)     │  Daily     │   (Tape/S3)     │                │   │
│  │   └─────────────────┘  Transfer  └─────────────────┘                │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 8.2 Velero 백업 구성

```yaml
# Velero Schedule - 전체 백업
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: llmflow-daily-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # 매일 02:00
  template:
    includedNamespaces:
      - llmflow-system
      - llmflow-infra
      - llmflow-inference
      - tenant-*
    excludedResources:
      - events
      - pods
    storageLocation: default
    volumeSnapshotLocations:
      - default
    ttl: 720h  # 30일
    snapshotVolumes: true
    hooks:
      resources:
        - name: postgresql-backup
          includedNamespaces:
            - llmflow-infra
          labelSelector:
            matchLabels:
              app: postgresql
          pre:
            - exec:
                container: postgresql
                command:
                  - /bin/sh
                  - -c
                  - pg_dump -U postgres -Fc dify > /backup/pre-backup.dump
                onError: Fail
                timeout: 30m

---
# Velero Schedule - 설정 백업 (더 자주)
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: llmflow-config-backup
  namespace: velero
spec:
  schedule: "0 */4 * * *"  # 4시간마다
  template:
    includedNamespaces:
      - llmflow-system
    includedResources:
      - configmaps
      - secrets
      - deployments
      - services
      - ingresses
    storageLocation: default
    ttl: 168h  # 7일
```

### 8.3 PostgreSQL PITR (Point-in-Time Recovery)

```yaml
# PostgreSQL Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-backup
  namespace: llmflow-infra
spec:
  schedule: "0 1 * * *"  # 매일 01:00
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: backup
              image: postgres:16-alpine
              env:
                - name: PGPASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgresql-credentials
                      key: password
                - name: PGHOST
                  value: postgresql-primary
              command:
                - /bin/sh
                - -c
                - |
                  set -e
                  BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
                  BACKUP_DIR=/backup/${BACKUP_DATE}
                  mkdir -p ${BACKUP_DIR}

                  # Full backup with pg_dump
                  for DB in dify keycloak langfuse mlflow; do
                    pg_dump -U postgres -Fc ${DB} > ${BACKUP_DIR}/${DB}.dump
                  done

                  # WAL archive backup
                  pg_basebackup -U replicator -D ${BACKUP_DIR}/basebackup -Ft -z -P

                  # Cleanup old backups (keep 30 days)
                  find /backup -type d -mtime +30 -exec rm -rf {} +

                  # Upload to MinIO
                  mc cp -r ${BACKUP_DIR} minio/backups/postgresql/
              volumeMounts:
                - name: backup-storage
                  mountPath: /backup
          volumes:
            - name: backup-storage
              persistentVolumeClaim:
                claimName: backup-pvc
          restartPolicy: OnFailure
```

### 8.4 재해 복구 절차

```yaml
# DR Runbook (ConfigMap으로 문서화)
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-runbook
  namespace: llmflow-system
data:
  recovery-procedure.md: |
    # LLMFlow 재해 복구 절차

    ## RTO/RPO 목표
    - RTO (Recovery Time Objective): 4시간
    - RPO (Recovery Point Objective): 1시간

    ## 복구 우선순위
    1. Kubernetes Control Plane
    2. 데이터베이스 (PostgreSQL, Redis)
    3. 벡터 DB (Milvus)
    4. 인증 시스템 (Keycloak)
    5. API Gateway (APISIX)
    6. 애플리케이션 (Dify)
    7. 추론 서비스 (vLLM)

    ## 단계별 복구 절차

    ### Phase 1: 인프라 복구 (0-1시간)
    ```bash
    # 1. etcd 복구
    etcdctl snapshot restore /backup/etcd-snapshot.db

    # 2. Kubernetes 클러스터 재구성
    kubeadm init --config kubeadm-config.yaml

    # 3. CNI 재설치
    kubectl apply -f cilium.yaml
    ```

    ### Phase 2: 데이터 복구 (1-2시간)
    ```bash
    # 1. PostgreSQL 복구
    pg_restore -U postgres -d dify /backup/dify.dump

    # 2. Redis 복구
    redis-cli -a ${REDIS_PASSWORD} --rdb /backup/dump.rdb

    # 3. Milvus 복구
    curl -X POST "http://milvus:19530/v1/vector/collections/restore" \
      -d '{"backup_name": "daily-backup"}'
    ```

    ### Phase 3: 서비스 복구 (2-4시간)
    ```bash
    # 1. Velero로 애플리케이션 복구
    velero restore create --from-backup llmflow-daily-backup

    # 2. DNS 레코드 업데이트
    # 3. 서비스 상태 확인
    kubectl get pods -A | grep -v Running

    # 4. 연결 테스트
    curl -v https://llmflow.company.local/health
    ```
```

---

## 9. 용량 산정

### 9.1 사용자 규모별 리소스 요구사항

| 사용자 규모 | 동시 사용자 | GPU (추론) | GPU (학습) | CPU 코어 | 메모리 | 스토리지 |
|------------|------------|-----------|-----------|---------|--------|---------|
| 소규모 (100명) | 20명 | A100x4 | A100x4 | 128 | 512GB | 20TB |
| 중규모 (500명) | 100명 | A100x16 | A100x8 | 384 | 2TB | 100TB |
| 대규모 (1000명+) | 200명+ | A100x32+ | H100x16 | 768+ | 4TB+ | 300TB+ |

### 9.2 모델별 리소스 요구사항

| 모델 크기 | GPU 메모리 | 권장 GPU | Tensor Parallel | 동시 요청 |
|----------|-----------|---------|-----------------|---------|
| 7-8B | 16GB+ | A100x1 | 1 | 50+ |
| 13B | 28GB+ | A100x1 | 1 | 30+ |
| 30-34B | 70GB+ | A100x2 | 2 | 20+ |
| 70B | 140GB+ | A100x4 | 4 | 10+ |
| 70B (Quantized) | 40GB+ | A100x1 | 1 | 15+ |

### 9.3 500명 사용자 기준 총 비용 추정

| 항목 | 사양 | 수량 | 예상 비용 (연간) |
|------|------|------|----------------|
| GPU 서버 (추론) | 8xA100 80GB | 4대 | - |
| GPU 서버 (학습) | 8xH100 80GB | 4대 | - |
| 애플리케이션 서버 | 56C/512GB | 6대 | - |
| 데이터베이스 서버 | 56C/512GB | 5대 | - |
| 벡터 DB 서버 | 56C/512GB | 6대 | - |
| 관리 서버 | 56C/256GB | 3대 | - |
| 네트워크 장비 | 100GbE/IB | - | - |
| 스토리지 | 500TB (총) | - | - |
| **전력/냉각** | 200kW+ | - | - |

> 실제 비용은 도입 시점의 하드웨어 가격과 협상에 따라 달라질 수 있습니다.

---

## 10. 체크리스트

### 10.1 배포 전 체크리스트

- [ ] 네트워크 설계 완료 (VLAN, 방화벽 규칙)
- [ ] 스토리지 프로비저닝 완료
- [ ] 인증서 발급 (내부 CA 또는 자체 서명)
- [ ] DNS 레코드 등록
- [ ] LDAP/AD 연동 테스트
- [ ] GPU 드라이버 및 CUDA 설치
- [ ] Kubernetes 클러스터 구성
- [ ] Helm 차트 값 검토
- [ ] 보안 스캔 완료
- [ ] 백업/복구 테스트

### 10.2 운영 체크리스트 (일일)

- [ ] 클러스터 상태 확인
- [ ] 디스크 사용량 확인 (>80% 경고)
- [ ] GPU 상태 확인
- [ ] 백업 성공 여부 확인
- [ ] 보안 알림 검토
- [ ] 사용자 피드백 검토

### 10.3 운영 체크리스트 (주간)

- [ ] 성능 메트릭 리뷰
- [ ] 용량 트렌드 분석
- [ ] 보안 패치 검토
- [ ] 인증서 만료 확인
- [ ] 로그 분석 및 이상 탐지
- [ ] 비용 최적화 검토

### 10.4 운영 체크리스트 (월간)

- [ ] 재해 복구 훈련
- [ ] 보안 감사
- [ ] 용량 계획 업데이트
- [ ] SLA 리뷰
- [ ] 기술 부채 검토
- [ ] 문서 업데이트

---

## 부록 A: 환경 변수 목록

```bash
# .env.production 예시

# ============ Database ============
POSTGRES_PASSWORD=<strong-password>
DB_PASSWORD=<strong-password>
REPLICATOR_PASSWORD=<strong-password>

# ============ Redis ============
REDIS_PASSWORD=<strong-password>

# ============ MinIO ============
MINIO_ACCESS_KEY=<access-key>
MINIO_SECRET_KEY=<secret-key>

# ============ Dify ============
DIFY_SECRET_KEY=<random-string-64>

# ============ Keycloak ============
KEYCLOAK_ADMIN_PASSWORD=<strong-password>
KEYCLOAK_DB_PASSWORD=<strong-password>
LDAP_BIND_PASSWORD=<ldap-password>

# ============ Langfuse ============
LANGFUSE_SECRET=<random-string-32>
LANGFUSE_SALT=<random-string-32>
LANGFUSE_DB_PASSWORD=<strong-password>

# ============ MLflow ============
MLFLOW_DB_PASSWORD=<strong-password>

# ============ Grafana ============
GRAFANA_PASSWORD=<strong-password>

# ============ OpenSearch ============
OPENSEARCH_PASSWORD=<strong-password>

# ============ Neo4j (GraphRAG) ============
NEO4J_PASSWORD=<strong-password>
NEO4J_AUTH=neo4j/<strong-password>

# ============ TEI (Embedding/Reranking) ============
TEI_EMBEDDING_MODEL=BAAI/bge-m3
TEI_RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# ============ LLaMA-Factory ============
LLAMA_FACTORY_ADMIN_PASSWORD=<strong-password>
```

---

## 부록 B: 유용한 명령어

```bash
# Kubernetes 상태 확인
kubectl get nodes -o wide
kubectl get pods -A | grep -v Running
kubectl top nodes
kubectl top pods -A

# GPU 상태 확인
kubectl exec -it <gpu-pod> -- nvidia-smi
kubectl get nodes -l nvidia.com/gpu.product -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable.'nvidia\.com/gpu'

# vLLM 상태 확인
curl http://vllm-service:8000/health
curl http://vllm-service:8000/v1/models

# TEI (임베딩/리랭킹) 상태 확인
curl http://tei-embedding:80/health
curl http://tei-reranker:80/health
curl -X POST http://tei-embedding:80/embed \
  -H "Content-Type: application/json" \
  -d '{"inputs": "test embedding"}'

# Neo4j (GraphRAG) 상태 확인
curl http://neo4j:7474/db/neo4j/cluster/available
cypher-shell -u neo4j -p ${NEO4J_PASSWORD} "CALL dbms.cluster.overview()"

# Unstructured (문서 파싱) 상태 확인
curl http://unstructured:8000/healthcheck
curl -X POST http://unstructured:8000/general/v0/general \
  -F "files=@test.pdf"

# LLaMA-Factory 상태 확인
curl http://llama-factory:7860/health

# Milvus 상태 확인
curl http://milvus-proxy:19530/v1/vector/collections

# 로그 확인
kubectl logs -f deployment/dify-api -n llmflow-system
kubectl logs -f deployment/tei-embedding -n llmflow-inference
kubectl logs -f deployment/neo4j -n llmflow-infra
stern 'dify-*' -n llmflow-system
stern 'vllm-*' -n llmflow-inference

# 메트릭 쿼리
curl 'http://prometheus:9090/api/v1/query?query=vllm_requests_total'
curl 'http://prometheus:9090/api/v1/query?query=tei_requests_total'
curl 'http://prometheus:9090/api/v1/query?query=neo4j_bolt_messages_done'

# Hybrid RAG 테스트
curl -X POST http://dify-api:5001/v1/chat-messages \
  -H "Authorization: Bearer ${API_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"query": "test query", "inputs": {}, "response_mode": "blocking"}'

# 백업 상태 확인
velero backup get
velero restore get
```

---

*문서 버전: 2.0*
*최종 업데이트: 2025년 1월*
*주요 변경: Ray 제거, TEI/Infinity 추가, Unstructured 추가, Neo4j 추가, LLaMA-Factory 도입*
