# ============================================
# LLMFlow Docker Compose - Full Stack
# Version: 2.0
# Services: 17 (Dify, vLLM, TEI, Milvus, Neo4j, etc.)
# ============================================

x-common-env: &common-env
  TZ: ${TZ:-Asia/Seoul}

x-healthcheck-defaults: &healthcheck-defaults
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

services:
  # ============================================
  # Phase 1: Infrastructure Services
  # ============================================

  etcd:
    image: quay.io/coreos/etcd:v3.5.11
    container_name: llmflow-etcd
    restart: unless-stopped
    environment:
      <<: *common-env
      ETCD_AUTO_COMPACTION_MODE: revision
      ETCD_AUTO_COMPACTION_RETENTION: "1000"
      ETCD_QUOTA_BACKEND_BYTES: "4294967296"
      ETCD_SNAPSHOT_COUNT: "50000"
    command:
      - etcd
      - --advertise-client-urls=http://etcd:2379
      - --listen-client-urls=http://0.0.0.0:2379
      - --data-dir=/etcd-data
      - --initial-advertise-peer-urls=http://etcd:2380
      - --listen-peer-urls=http://0.0.0.0:2380
      - --initial-cluster=default=http://etcd:2380
      - --name=default
    volumes:
      - etcd-data:/etcd-data
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "etcdctl", "endpoint", "health"]
    networks:
      - llmflow-data

  postgresql:
    image: postgres:16-alpine
    container_name: llmflow-postgresql
    restart: unless-stopped
    environment:
      <<: *common-env
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgresql-data:/var/lib/postgresql/data
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
    ports:
      - "5432:5432"
    networks:
      - llmflow-data

  redis:
    image: redis:7-alpine
    container_name: llmflow-redis
    restart: unless-stopped
    environment:
      <<: *common-env
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes
    volumes:
      - redis-data:/data
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
    ports:
      - "6379:6379"
    networks:
      - llmflow-data

  minio:
    image: minio/minio:latest
    container_name: llmflow-minio
    restart: unless-stopped
    environment:
      <<: *common-env
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - llmflow-data

  # ============================================
  # Phase 2: Data Stores
  # ============================================

  milvus-standalone:
    image: milvusdb/milvus:v2.4.0
    container_name: llmflow-milvus
    restart: unless-stopped
    environment:
      <<: *common-env
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MINIO_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      MINIO_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    command: ["milvus", "run", "standalone"]
    volumes:
      - milvus-data:/var/lib/milvus
    depends_on:
      etcd:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      start_period: 120s
    ports:
      - "19530:19530"
      - "9091:9091"
    networks:
      - llmflow-data

  neo4j:
    image: neo4j:5.15.0-enterprise
    container_name: llmflow-neo4j
    restart: unless-stopped
    environment:
      <<: *common-env
      NEO4J_AUTH: ${NEO4J_AUTH}
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_dbms_memory_heap_initial__size: "1G"
      NEO4J_dbms_memory_heap_max__size: "2G"
      NEO4J_dbms_memory_pagecache_size: "1G"
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*,gds.*"
      NEO4J_dbms_security_procedures_allowlist: "apoc.*,gds.*"
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
      - neo4j-plugins:/plugins
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "wget -q --spider http://localhost:7474 || exit 1"]
      start_period: 120s
      interval: 15s
      retries: 10
    ports:
      - "7474:7474"
      - "7687:7687"
    networks:
      - llmflow-data

  # ============================================
  # Phase 3: Auth & Gateway
  # ============================================

  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    container_name: llmflow-keycloak
    restart: unless-stopped
    environment:
      <<: *common-env
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgresql:5432/keycloak
      KC_DB_USERNAME: ${POSTGRES_USER}
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD}
      KC_HOSTNAME_STRICT: "false"
      KC_PROXY: edge
      KC_HTTP_ENABLED: "true"
    command: start-dev
    depends_on:
      postgresql:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/localhost/8080 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | grep -q '200 OK'"]
      start_period: 60s
    ports:
      - "8080:8080"
    networks:
      - llmflow-auth
      - llmflow-data

  apisix:
    image: apache/apisix:3.8.0-debian
    container_name: llmflow-apisix
    restart: unless-stopped
    environment:
      <<: *common-env
    volumes:
      - ./configs/apisix/config.yaml:/usr/local/apisix/conf/config.yaml:ro
      - ./configs/apisix/apisix.yaml:/usr/local/apisix/conf/apisix.yaml:ro
    depends_on:
      etcd:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:9080/apisix/status"]
      interval: 10s
    ports:
      - "9080:9080"
      - "9443:9443"
      - "9180:9180"
    networks:
      - llmflow-frontend
      - llmflow-backend
      - llmflow-auth
      - llmflow-data

  # ============================================
  # Phase 4: Core Platform (Dify)
  # ============================================

  dify-api:
    build:
      context: ./dify-patches
      dockerfile: Dockerfile
    image: llmflow-dify-api:0.11.0-custom
    container_name: llmflow-dify-api
    restart: unless-stopped
    environment:
      <<: *common-env
      MODE: api
      LOG_LEVEL: INFO
      SECRET_KEY: ${DIFY_SECRET_KEY}
      INIT_PASSWORD: ${DIFY_INIT_PASSWORD}
      # Session settings - extended for better UX
      ACCESS_TOKEN_EXPIRE_MINUTES: 1440
      REFRESH_TOKEN_EXPIRE_DAYS: 30
      CONSOLE_WEB_URL: http://localhost:3033
      SERVICE_API_URL: http://localhost:5001
      APP_WEB_URL: http://localhost:3033
      CONSOLE_CORS_ALLOW_ORIGINS: http://localhost:3033,http://127.0.0.1:3033,*
      WEB_API_CORS_ALLOW_ORIGINS: http://localhost:3033,http://127.0.0.1:3033,*
      # File upload limits
      UPLOAD_FILE_SIZE_LIMIT: 100
      UPLOAD_FILE_BATCH_LIMIT: 10
      # Database
      DB_HOST: postgresql
      DB_PORT: 5432
      DB_USERNAME: ${POSTGRES_USER}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_DATABASE: dify
      # Redis
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_DB: 0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1
      # Storage
      STORAGE_TYPE: s3
      S3_ENDPOINT: http://minio:9000
      S3_BUCKET_NAME: dify
      S3_ACCESS_KEY: ${MINIO_ROOT_USER}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      S3_REGION: us-east-1
      S3_USE_SSL: "false"
      # Vector Store
      VECTOR_STORE: milvus
      MILVUS_URI: http://milvus-standalone:19530
      MILVUS_USER: root
      MILVUS_PASSWORD: Milvus
      # ETL - Use Unstructured for better PDF parsing with OCR
      ETL_TYPE: Unstructured
      UNSTRUCTURED_API_URL: http://unstructured:8000/general/v0/general
      # Observability
      LANGFUSE_PUBLIC_KEY: ""
      LANGFUSE_SECRET_KEY: ""
      LANGFUSE_HOST: http://langfuse:3000
    volumes:
      - dify-storage:/app/api/storage
      # Patch for Korean OCR support in PDF extraction
      - ./dify-patches/unstructured_pdf_extractor.py:/app/api/core/rag/extractor/unstructured/unstructured_pdf_extractor.py:ro
    depends_on:
      postgresql:
        condition: service_healthy
      unstructured:
        condition: service_started
      redis:
        condition: service_healthy
      milvus-standalone:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
    ports:
      - "5001:5001"
    networks:
      - llmflow-backend
      - llmflow-data
      - llmflow-inference

  dify-worker:
    image: llmflow-dify-api:0.11.0-custom
    container_name: llmflow-dify-worker
    restart: unless-stopped
    environment:
      <<: *common-env
      MODE: worker
      LOG_LEVEL: INFO
      SECRET_KEY: ${DIFY_SECRET_KEY}
      # Celery worker concurrency (reduce for lower resource usage)
      CELERY_WORKER_CONCURRENCY: 2
      CELERY_WORKER_PREFETCH_MULTIPLIER: 1
      # Database
      DB_HOST: postgresql
      DB_PORT: 5432
      DB_USERNAME: ${POSTGRES_USER}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_DATABASE: dify
      # Redis
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_DB: 0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1
      # Storage
      STORAGE_TYPE: s3
      S3_ENDPOINT: http://minio:9000
      S3_BUCKET_NAME: dify
      S3_ACCESS_KEY: ${MINIO_ROOT_USER}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      S3_REGION: us-east-1
      S3_USE_SSL: "false"
      # Vector Store
      VECTOR_STORE: milvus
      MILVUS_URI: http://milvus-standalone:19530
      MILVUS_USER: root
      MILVUS_PASSWORD: Milvus
      # ETL - Use Unstructured for better PDF parsing with OCR
      ETL_TYPE: Unstructured
      UNSTRUCTURED_API_URL: http://unstructured:8000/general/v0/general
    volumes:
      - dify-storage:/app/api/storage
      # Patch for Korean OCR support in PDF extraction
      - ./dify-patches/unstructured_pdf_extractor.py:/app/api/core/rag/extractor/unstructured/unstructured_pdf_extractor.py:ro
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2'
    depends_on:
      - dify-api
    networks:
      - llmflow-backend
      - llmflow-data
      - llmflow-inference

  # Original Dify Web - Disabled in favor of llmflow-ui
  # dify-web:
  #   image: langgenius/dify-web:0.11.0
  #   container_name: llmflow-dify-web
  #   ...

  # Nginx proxy for Dify API (handles CORS)
  dify-nginx:
    image: nginx:alpine
    container_name: llmflow-dify-nginx
    restart: unless-stopped
    volumes:
      - ./configs/nginx/dify-proxy.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - dify-api
    ports:
      - "5002:80"
    networks:
      - llmflow-frontend
      - llmflow-backend

  # LLMFlow UI - Custom frontend (replaces dify-web)
  llmflow-ui:
    image: node:20-alpine
    container_name: llmflow-ui
    working_dir: /app
    command: sh -c "npm install && npm run dev"
    restart: unless-stopped
    environment:
      <<: *common-env
      NEXT_PUBLIC_API_URL: http://localhost:5002
      NEXT_PUBLIC_APP_NAME: LLMFlow
      DIFY_API_URL: http://dify-nginx:80
    depends_on:
      - dify-nginx
    volumes:
      - ../ui:/app
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000"]
    ports:
      - "3033:3000"
    networks:
      - llmflow-frontend
      - llmflow-backend

  # ============================================
  # Phase 5: Inference Services
  # ============================================

  vllm:
    image: vllm/vllm-openai:v0.6.4.post1
    container_name: llmflow-vllm
    restart: unless-stopped
    environment:
      <<: *common-env
      VLLM_API_KEY: ${VLLM_API_KEY}
      HF_HOME: /models
      # Llama 4 MoE requires NCCL for multi-GPU
      NCCL_P2P_DISABLE: "0"
      NCCL_IB_DISABLE: "0"
    command:
      - --model
      - ${VLLM_MODEL_PATH:-/models/llama-4-mini}
      - --api-key
      - ${VLLM_API_KEY}
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --gpu-memory-utilization
      - "${VLLM_GPU_MEMORY_UTILIZATION:-0.9}"
      - --max-model-len
      - "${VLLM_MAX_MODEL_LEN:-32768}"
      - --tensor-parallel-size
      - "${VLLM_TENSOR_PARALLEL:-1}"
      - --enable-prefix-caching
      - --trust-remote-code
      # FP8 for H200 optimization (Prod only)
      # - --quantization
      # - fp8
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${VLLM_GPU_COUNT:-1}
              capabilities: [gpu]
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      start_period: 180s
    ports:
      - "8000:8000"
    networks:
      - llmflow-inference

  tei-embedding:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.6
    container_name: llmflow-tei-embedding
    restart: unless-stopped
    environment:
      <<: *common-env
      HF_HUB_URL: https://huggingface.co
      HF_ENDPOINT: https://huggingface.co
    command:
      - --model-id
      - ${TEI_EMBEDDING_MODEL:-BAAI/bge-m3}
      - --port
      - "80"
      - --max-client-batch-size
      - "32"
      - --max-batch-tokens
      - "16384"
    volumes:
      - tei-models:/data
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      start_period: 120s
    ports:
      - "8083:80"
    networks:
      - llmflow-inference

  tei-reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:1.6
    container_name: llmflow-tei-reranker
    restart: unless-stopped
    environment:
      <<: *common-env
      HF_HUB_URL: https://huggingface.co
      HF_ENDPOINT: https://huggingface.co
    command:
      - --model-id
      - ${TEI_RERANKER_MODEL:-BAAI/bge-reranker-v2-m3}
      - --port
      - "80"
      - --max-client-batch-size
      - "32"
    volumes:
      - tei-models:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      start_period: 120s
    ports:
      - "8081:80"
    networks:
      - llmflow-inference

  unstructured:
    image: quay.io/unstructured-io/unstructured-api:0.0.80
    container_name: llmflow-unstructured
    restart: unless-stopped
    environment:
      <<: *common-env
      UNSTRUCTURED_PARALLEL_MODE_ENABLED: "true"
      UNSTRUCTURED_PARALLEL_MODE_THREADS: "2"
      UNSTRUCTURED_PARALLEL_MODE_URL: "http://localhost:8000/general/v0/general"
      # OCR and hi_res strategy for scanned PDFs
      UNSTRUCTURED_PDF_STRATEGY: "hi_res"
      UNSTRUCTURED_HI_RES_MODEL_NAME: "yolox"
      UNSTRUCTURED_OCR_LANGUAGES: "kor+eng"
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthcheck"]
    ports:
      - "8001:8000"
    networks:
      - llmflow-inference

  # ============================================
  # Phase 5.5: GraphRAG Service
  # ============================================

  graphrag:
    build:
      context: ../services/graphrag
      dockerfile: Dockerfile
    container_name: llmflow-graphrag
    restart: unless-stopped
    environment:
      <<: *common-env
      # Neo4j connection
      GRAPHRAG_NEO4J_URI: bolt://neo4j:7687
      GRAPHRAG_NEO4J_USER: neo4j
      GRAPHRAG_NEO4J_PASSWORD: ${NEO4J_PASSWORD:-neo4j_llmflow}
      # Milvus connection
      GRAPHRAG_MILVUS_HOST: milvus-standalone
      GRAPHRAG_MILVUS_PORT: 19530
      # LLM (vLLM) - OpenAI compatible API
      GRAPHRAG_LLM_API_BASE: http://vllm:8000/v1
      GRAPHRAG_LLM_API_KEY: ${VLLM_API_KEY}
      GRAPHRAG_LLM_MODEL: ${VLLM_MODEL_NAME:-TinyLlama-1.1B-Chat-v1.0}
      # Embedding (TEI)
      GRAPHRAG_EMBEDDING_API_BASE: http://tei-embedding:80
      # Search parameters
      GRAPHRAG_VECTOR_TOP_K: 20
      GRAPHRAG_GRAPH_MAX_DEPTH: 3
      GRAPHRAG_RRF_K: 60
      # Dify PostgreSQL (for direct segment access)
      DIFY_DB_HOST: postgresql
      DIFY_DB_PORT: 5432
      DIFY_DB_USER: ${POSTGRES_USER:-llmflow}
      DIFY_DB_PASSWORD: ${POSTGRES_PASSWORD:-postgres_llmflow}
      DIFY_DB_NAME: ${DIFY_DB_NAME:-dify}
      # S3/MinIO for PDF access (Docling)
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minio_llmflow}
      S3_BUCKET_NAME: ${S3_BUCKET_NAME:-dify}
    volumes:
      # Persist EasyOCR and Docling model cache
      - graphrag-models:/app/.easyocr
      - graphrag-docling:/app/.docling
    depends_on:
      neo4j:
        condition: service_healthy
      milvus-standalone:
        condition: service_healthy
      postgresql:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    ports:
      - "8082:8080"
    networks:
      - llmflow-backend
      - llmflow-data
      - llmflow-inference

  # ============================================
  # Phase 6: LLMOps & Observability
  # ============================================

  llama-factory:
    image: hiyouga/llama-factory:latest
    container_name: llmflow-llama-factory
    restart: unless-stopped
    environment:
      <<: *common-env
      GRADIO_SERVER_NAME: ${LLAMA_FACTORY_GRADIO_SERVER_NAME:-0.0.0.0}
      GRADIO_SERVER_PORT: ${LLAMA_FACTORY_GRADIO_SERVER_PORT:-7860}
    command: llamafactory-cli webui
    volumes:
      - ../models:/app/models
      - llama-factory-data:/app/data
      - llama-factory-output:/app/output
      - llama-factory-cache:/root/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "7860:7860"
    networks:
      - llmflow-backend
      - llmflow-data

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    container_name: llmflow-mlflow
    restart: unless-stopped
    environment:
      <<: *common-env
      MLFLOW_BACKEND_STORE_URI: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/mlflow
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://mlflow-artifacts
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    command: sh -c "pip install psycopg2-binary && mlflow server --host 0.0.0.0 --port 5000"
    depends_on:
      postgresql:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
    ports:
      - "5000:5000"
    networks:
      - llmflow-monitoring
      - llmflow-data

  langfuse:
    image: langfuse/langfuse:2
    container_name: llmflow-langfuse
    restart: unless-stopped
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/langfuse
      NEXTAUTH_URL: http://localhost:3001
      NEXTAUTH_SECRET: ${LANGFUSE_NEXT_AUTH_SECRET}
      SALT: ${LANGFUSE_SALT}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "true"
    depends_on:
      postgresql:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/public/health"]
    ports:
      - "3001:3000"
    networks:
      - llmflow-monitoring
      - llmflow-data
      - llmflow-backend

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: llmflow-prometheus
    restart: unless-stopped
    environment:
      <<: *common-env
    volumes:
      - ./configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME:-15d}'
      - '--web.enable-lifecycle'
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:9090/-/healthy"]
    ports:
      - "9090:9090"
    networks:
      - llmflow-monitoring

  grafana:
    image: grafana/grafana:10.2.3
    container_name: llmflow-grafana
    restart: unless-stopped
    environment:
      <<: *common-env
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./configs/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    depends_on:
      - prometheus
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
    ports:
      - "3032:3000"
    networks:
      - llmflow-monitoring

# ============================================
# Networks
# ============================================

networks:
  llmflow-frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.1.0/24

  llmflow-backend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.2.0/24

  llmflow-data:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.3.0/24

  llmflow-inference:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.4.0/24

  llmflow-auth:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.5.0/24

  llmflow-monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.6.0/24

# ============================================
# Volumes
# ============================================

volumes:
  etcd-data:
  postgresql-data:
  redis-data:
  minio-data:
  milvus-data:
  neo4j-data:
  neo4j-logs:
  neo4j-plugins:
  dify-storage:
  tei-models:
  llama-factory-data:
  llama-factory-output:
  llama-factory-cache:
  prometheus-data:
  grafana-data:
  graphrag-models:
  graphrag-docling:
